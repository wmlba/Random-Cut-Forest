{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries and specify Amazon S3 Bucket\n",
    "\n",
    "* `bucket` - An S3 bucket accessible by this account.\n",
    "* `prefix` - The location in the bucket where this notebook's input and output data will be stored. (The default value is sufficient.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "isConfigCell": true
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import botocore\n",
    "import sagemaker\n",
    "import sys\n",
    "\n",
    "#Specify an S3 bucket that will be used for the training process.\n",
    "bucket = 'sagemaker-walebadr'   # <--- specify a bucket you have access to\n",
    "prefix = 'sagemaker/rcf-benchmarks'\n",
    "execution_role = sagemaker.get_execution_role()\n",
    "\n",
    "\n",
    "# check if the bucket exists\n",
    "try:\n",
    "    boto3.Session().client('s3').head_bucket(Bucket=bucket)\n",
    "except botocore.exceptions.ParamValidationError as e:\n",
    "    print('Hey! You either forgot to specify your S3 bucket'\n",
    "          ' or you gave your bucket an invalid name!')\n",
    "except botocore.exceptions.ClientError as e:\n",
    "    if e.response['Error']['Code'] == '403':\n",
    "        print(\"Hey! You don't have permission to access the bucket, {}.\".format(bucket))\n",
    "    elif e.response['Error']['Code'] == '404':\n",
    "        print(\"Hey! Your bucket, {}, doesn't exist!\".format(bucket))\n",
    "    else:\n",
    "        raise\n",
    "else:\n",
    "    print('Training input/output will be stored in: s3://{}/{}'.format(bucket, prefix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain and Inspect Sample Data\n",
    "\n",
    "\n",
    "Our data comes from the Numenta Anomaly Benchmark (NAB) [[1](https://github.com/numenta/NAB/blob/master/data/realKnownCause/machine_temperature_system_failure.csv)]. The data records the temperature sensor data from an internal component of a large industrial machine. The data collected over the course of 3 months aggregated into 5-minute buckets.\n",
    "\n",
    "> https://github.com/numenta/NAB/blob/master/data/realKnownCause/machine_temperature_system_failure.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "\n",
    "data_filename = '2018.csv'#'machine_temperature_system_failure.csv'\n",
    "data_source = 'https://raw.githubusercontent.com/numenta/NAB/master/data/realKnownCause/machine_temperature_system_failure.csv'\n",
    "\n",
    "#urllib.request.urlretrieve(data_source, data_filename)\n",
    "temp_data = pd.read_csv(data_filename, delimiter=',')\n",
    "prediction_data = pd.read_csv(\"2018.csv\", delimiter=',')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Inspection\n",
    "\n",
    "Before training any models it is important to inspect our data, first. Perhaps there are some underlying patterns or structures that we could provide as \"hints\" to the model or maybe there is some noise that we could pre-process away. The raw data looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the dataset\n",
    "\n",
    "If you're running this notebook on Amazon Sagemaker, please install the `bokeh` library manually in the terminal of this notebook machine. or you can run `!sudo pip install bokeh` in one of the blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bokeh\n",
    "import bokeh.io\n",
    "from bokeh.models import HoverTool\n",
    "bokeh.io.output_notebook()\n",
    "from bokeh.plotting import figure, show, output_file, ColumnDataSource\n",
    "from bokeh.models.formatters import DatetimeTickFormatter\n",
    "import bokeh.palettes\n",
    "temp_data['timestamp'] = pd.to_datetime(temp_data['timestamp'])\n",
    "output_file(\"datetime.html\")\n",
    "date = temp_data['timestamp']\n",
    "temp = temp_data['value']\n",
    "\n",
    "x = date.values\n",
    "y = temp.values\n",
    "score = []\n",
    "\n",
    "string_x = list(map(str, x))\n",
    "source = ColumnDataSource(temp_data)\n",
    "source.add(temp_data['timestamp'].apply(lambda d: d.strftime('%Y-%m-%d %H:%M:%S')), 'event_date_formatted')\n",
    "#Hover Tool\n",
    "\n",
    "hover = HoverTool(\n",
    "    names = [\"temp\"],\n",
    "    tooltips=[\n",
    "        ( 'date',   '@event_date_formatted'),\n",
    "        ( 'temperature',  '$y' ),        \n",
    "    ],\n",
    "\n",
    "\n",
    "    # display a tooltip whenever the cursor is vertically in line with a glyph\n",
    "    mode='vline'\n",
    ")\n",
    "\n",
    "p = figure(x_axis_type='datetime', plot_width=900, plot_height=400, tools=[hover, 'pan','wheel_zoom','box_zoom','reset']) \n",
    "p.line( name=\"temp\", x='timestamp',y= 'value',source=source, line_width=2,color='navy', alpha=0.5)\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store Data on S3\n",
    "\n",
    "The Random Cut Forest Algorithm accepts data in [RecordIO](https://mxnet.apache.org/api/python/io/io.html#module-mxnet.recordio) [Protobuf](https://developers.google.com/protocol-buffers/) format. The SageMaker Python API provides helper functions for easily converting your data into this format. Below we convert the temperature sensor data and upload it to the `bucket + prefix` Amazon S3 destination specified at the beginning of this notebook in the [Setup AWS Credentials](#Setup-AWS-Credentials) section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_and_upload_training_data(ndarray, bucket, prefix, filename='data.pbr'):\n",
    "    import boto3\n",
    "    import os\n",
    "    from sagemaker.amazon.common import numpy_to_record_serializer\n",
    "    \n",
    "    # convert numpy array to Protobuf RecordIO format\n",
    "    serializer = numpy_to_record_serializer()\n",
    "    buffer = serializer(ndarray)\n",
    "    \n",
    "    # Upload to S3\n",
    "    s3_object = os.path.join(prefix, 'train', filename)\n",
    "    boto3.Session().resource('s3').Bucket(bucket).Object(s3_object).upload_fileobj(buffer)\n",
    "    \n",
    "    s3_path = 's3://{}/{}'.format(bucket, s3_object)\n",
    "    return s3_path\n",
    "\n",
    "# RCV only works on an array of values.\n",
    "s3_train_data = convert_and_upload_training_data(\n",
    "    temp_data.value.as_matrix().reshape(-1,1),\n",
    "    bucket,\n",
    "    prefix)\n",
    "print('Uploaded data to {}'.format(s3_train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "***\n",
    "\n",
    "We have created a training data set and uploaded it to S3. Next, we configure a SageMaker training job to use the Random Cut Forest (RCF) algorithm on said training data.\n",
    "\n",
    "The first step is to specify the location of the Docker image containing the SageMaker Random Cut Forest algorithm. In order to minimize communication latency, we provide containers for each AWS region in which SageMaker is available. The code below automatically chooses an algorithm container based on the current region; that is, the region in which this notebook is run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters and Job Definition\n",
    "\n",
    "Particular to a SageMaker RCF training job are the following hyperparameters:\n",
    "\n",
    "* **`num_samples_per_tree`** - the number randomly sampled data points sent to each tree. As a general rule, `1/num_samples_per_tree` should approximate the the estimated ratio of anomalies to normal points in the dataset.\n",
    "* **`num_trees`** - the number of trees to create in the forest. Each tree learns a separate model from different samples of data. The full forest model uses the mean predicted anomaly score from each constituent tree.\n",
    "* **`feature_dim`** - the dimension of each data point.\n",
    "\n",
    "In addition to these RCF model hyperparameters, we provide additional parameters defining things like the EC2 instance type on which training will run, the S3 bucket containing the data, and the AWS access role. Note that,\n",
    "\n",
    "* Recommended instance type: `ml.m4`, `ml.c4`, or `ml.c5`\n",
    "* Current limitations:\n",
    "  * The RCF algorithm does not take advantage of GPU hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "session = sagemaker.Session()\n",
    "\n",
    "# Specify the location of the training container\n",
    "container = '382416733822.dkr.ecr.us-east-1.amazonaws.com/randomcutforest:latest'\n",
    "\n",
    "# specify general training job information\n",
    "rcf = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    execution_role,\n",
    "    input_mode='File',\n",
    "    output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.m4.xlarge',\n",
    "    sagemaker_session=session,\n",
    ")\n",
    "\n",
    "# set algorithm-specific hyperparameters\n",
    "rcf.set_hyperparameters(\n",
    "    num_samples_per_tree = 500,\n",
    "    num_trees = 100,\n",
    "    feature_dim = 1,\n",
    ")\n",
    "\n",
    "# RCF training requires sharded data. See documentation for\n",
    "# more information.\n",
    "\n",
    "s3_train_input = sagemaker.session.s3_input(\n",
    "    s3_train_data,\n",
    "    distribution='ShardedByS3Key',\n",
    "    content_type='application/x-recordio-protobuf',\n",
    ")\n",
    "\n",
    "\n",
    "# run the training job on input data stored in S3\n",
    "rcf.fit({'train': s3_train_input})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you see the message\n",
    "\n",
    "> `===== Job Complete =====`\n",
    "\n",
    "at the bottom of the output logs then that means training successfully completed and the output RCF model was stored in the specified output path. You can also view information about and the status of a training job using the AWS SageMaker console. Just click on the \"Jobs\" tab and select training job matching the training job name, below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training job name: {}'.format(rcf.latest_training_job.job_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "***\n",
    "\n",
    "A trained Random Cut Forest model does nothing on its own. We now want to use the model we computed to perform inference on data. In this case, it means computing anomaly scores from input time series data points.\n",
    "\n",
    "We create an inference endpoint using the SageMaker Python SDK `deploy()` function from the job we defined above. We specify the instance type where inference is computed as well as an initial number of instances to spin up. We recommend using the `ml.c5` instance type as it provides the fastest inference time at the lowest cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcf_inference = rcf.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m4.xlarge',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Congratulations! You now have a functioning SageMaker RCF inference endpoint. You can confirm the endpoint configuration and status by navigating to the \"Endpoints\" tab in the AWS SageMaker console and selecting the endpoint matching the endpoint name, below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Endpoint name: {}'.format(rcf_inference.endpoint))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Serialization/Deserialization\n",
    "\n",
    "We can pass data in a variety of formats to our inference endpoint. In this example we will demonstrate passing CSV-formatted data. Other available formats are JSON-formatted and RecordIO Protobuf. We make use of the SageMaker Python SDK utilities `csv_serializer` and `json_deserializer` when configuring the inference endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import csv_serializer, json_deserializer\n",
    "\n",
    "rcf_inference.content_type = 'text/csv'\n",
    "rcf_inference.serializer = csv_serializer\n",
    "rcf_inference.deserializer = json_deserializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pass the training dataset, in CSV format, to the inference endpoint so we can automatically detect the anomalies we saw with our eyes in the plots, above. Note that the serializer and deserializer will automatically take care of the datatype conversion from Numpy NDArrays.\n",
    "\n",
    "For starters, let's only pass in the first six datapoints so we can see what the output looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "prediction_data = pd.read_csv(\"2018.csv\", delimiter=',')\n",
    "prediction_data_numpy = prediction_data.value.as_matrix().reshape(-1,1)\n",
    "results = rcf_inference.predict(prediction_data_numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Anomaly Scores\n",
    "\n",
    "Now, let's compute and plot the anomaly scores from the entire temperature dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.models import Range1d, LinearAxis\n",
    "def prediction(data):\n",
    "    prediction_data = data\n",
    "    prediction_data['timestamp'] = pd.to_datetime(prediction_data['timestamp'])\n",
    "    output_file(\"datetime.html\")\n",
    "    temp = prediction_data['value']\n",
    "    scores = prediction_data['score']\n",
    "\n",
    "    hover = HoverTool(\n",
    "        names = [\"temp\",\"score\"],\n",
    "        tooltips=[\n",
    "            ( 'date',   '@event_date_formatted'),\n",
    "            ( 'temp',  '@temp_y' ), \n",
    "           ( 'Score',  '@score_y' ), \n",
    "        ],\n",
    "    )\n",
    "    source = ColumnDataSource(prediction_data)\n",
    "    source.add(prediction_data['timestamp'].apply(lambda d: d.strftime('%Y-%m-%d %H:%M:%S')), 'event_date_formatted')\n",
    "    source.add(temp, 'temp_y')\n",
    "    source.add(scores, 'score_y')\n",
    "\n",
    "    p = figure(x_axis_type='datetime', plot_width=1000, plot_height=350, tools=[hover, 'pan','wheel_zoom','box_zoom','reset']) \n",
    "    p.line( name = \"temp\", x='timestamp',y= 'value',source=source, line_width=2,color='navy', alpha=0.5, legend=[\"Temperature\"])\n",
    "    p.extra_y_ranges = {\"Anomaly\": Range1d(start=0, end=10)}\n",
    "    p.add_layout(LinearAxis(y_range_name=\"Anomaly\"), 'right')\n",
    "    p.line( name = \"score\", x='timestamp',y='score',source=source, line_width=2,color='red', alpha=0.5, y_range_name=\"Anomaly\",legend=[\"Score\"])\n",
    "    p.legend.location = \"top_left\"\n",
    "    p.legend.click_policy=\"hide\"\n",
    "    p.title.text = \"Anomaly Detection for Device Temperature\"\n",
    "\n",
    "\n",
    "\n",
    "    #select the highest anomaly scores\n",
    "    score_mean = prediction_data['score'].mean()\n",
    "    score_std = prediction_data['score'].std()\n",
    "    score_cutoff = score_mean + 3*score_std\n",
    "    print(score_cutoff)\n",
    "    anomalies = prediction_data[prediction_data['score'] > score_cutoff]\n",
    "    sorted_anomalies = anomalies.sort_values(by=['score'], ascending=False)\n",
    "    print(sorted_anomalies)\n",
    "    source = ColumnDataSource(prediction_data)\n",
    "    #p.circle( sorted_anomalies['timestamp'],sorted_anomalies['score'], line_width=8,color='black', alpha=0.5, y_range_name=\"Anomaly\")\n",
    "    p.circle( sorted_anomalies['timestamp'],sorted_anomalies['value'], line_width=5,color='black')\n",
    "\n",
    "\n",
    "    show(p)\n",
    "    \n",
    "def process_data(file):\n",
    "    prediction_data = pd.read_csv(file, delimiter=',')\n",
    "    prediction_data_numpy = prediction_data.value.as_matrix().reshape(-1,1)\n",
    "    results = rcf_inference.predict(prediction_data_numpy)\n",
    "    scores = [datum['score'] for datum in results['scores']]\n",
    "    prediction_data['score'] = pd.Series(scores, index=prediction_data.index)\n",
    "    return prediction_data\n",
    "#prediction(prediction_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make prediction on the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prediction(process_data(\"2018.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make prediction on a new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction(process_data(\"temp_predictions.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the anomaly score spikes where our eyeball-norm method suggests there is an anomalous data point as well as in some places where our eyeballs are not as accurate.\n",
    "\n",
    "Below we print and plot any data points with scores greater than 3 standard deviations (approx 99.9th percentile) from the mean score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first anomaly was a planned shutdown. The third anomaly was a catastrophic system failure. The other few anomalies in the middle, a subtle but observable change in the behavior, indicated the actual onset of the problem that led to the eventual system failure.\n",
    "\n",
    "* `2013-12-16` - A planned shutdown\n",
    "* `2014-02-01` - Subtle anomalies that led to the failure.\n",
    "* `2014-02-08` - Catastrophic System Failure\n",
    "\n",
    "Note that our algorithm managed to capture these events along with quite a few others. Below we add these anomalies to the score plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the current hyperparameter choices we see that the three-standard-deviation threshold, while able to capture the known anomalies as well as the ones apparent in the ridership plot, is rather sensitive to fine-grained peruturbations and anomalous behavior. Adding trees to the SageMaker RCF model could smooth out the results as well as using a larger data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop and Delete the Endpoint\n",
    "\n",
    "Finally, we should delete the endpoint before we close the notebook.\n",
    "\n",
    "To do so execute the cell below. Alternately, you can navigate to the \"Endpoints\" tab in the SageMaker console, select the endpoint with the name stored in the variable `endpoint_name`, and select \"Delete\" from the \"Actions\" dropdown menu. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.Session().delete_endpoint(rcf_inference.endpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
